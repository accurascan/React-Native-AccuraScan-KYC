import UIKit
import AccuraOCR
import Photos
import PhotosUI
//import SVProgressHUD
//import ProgressHUD

//View controller for face match.
class FMController: UIViewController, UIImagePickerControllerDelegate, UINavigationControllerDelegate,PHPickerViewControllerDelegate, UIAdaptivePresentationControllerDelegate {
    
    
    @IBOutlet var mainView: UIView!
    
    //NEW
    var imagePicker = UIImagePickerController()
    var arrDocument: [UIImage] = [UIImage]()
    var selectFirstImage = false
    var facematch = Facematch()
    var liveness = Liveness()
    var gallery1 = false
    var gallery2 = false
    var faceMatch1 = false
    var facematch2 = false
    
    //OLD
    var faceArgs:NSDictionary = [:]
    var callBack: RCTResponseSenderBlock? = nil
    var reactViewController:UIViewController? = nil
    var audioPath: URL? = nil
    var isFacematchDone = false
    var isCalledCallBack = false
    var win: UIWindow? = nil
    
    func closeMe() {
        //        self.win!.rootViewController = reactViewController!
        DispatchQueue.main.async {
            self.dismiss(animated: true) {}
        }
        
    }
    
    override var supportedInterfaceOrientations: UIInterfaceOrientationMask {
        
        return .portrait
    }
    
    override func viewDidLoad() {
        mainView.isHidden = true
        super.viewDidLoad()
        //        print(gl.face1)
        //        print(gl.face2)
        
        imagePicker.delegate = self
        
        if gallery1 {
            imageGallery1()
        } else if gallery2{
            imageGallery2()
        } else if faceMatch1 {
            imageCamera1()
        } else if facematch2 {
            imageCamera2()
        }
    }
    
    override func viewDidAppear(_ animated: Bool) {
        
        
    }
    
    override func viewWillAppear(_ animated: Bool) {
        facematch = Facematch.init()
//        liveness = Liveness.init()
//        liveness.setBlurPercentage(80)
//        liveness.setGlarePercentage(-1,-1)
//        facematch.setBackGroundColor("#C4C4C5")
//        facematch.setCloseIconColor("#000000")
//        facematch.setFeedbackBackGroundColor("#C4C4C5")
//        facematch.setFeedbackTextColor("#000000")
        
        
        if faceArgs["backGroundColor"] != nil{
            facematch.setBackGroundColor(faceArgs["backGroundColor"] as! String)
        }else{
            facematch.setBackGroundColor("#C4C4C5")
        }
        if faceArgs["closeIconColor"] != nil{
            facematch.setCloseIconColor(faceArgs["closeIconColor"] as! String)
        }else{
            facematch.setCloseIconColor("#000000")
        }
        if faceArgs["feedbackBackGroundColor"] != nil{
            facematch.setFeedbackBackGroundColor(faceArgs["feedbackBackGroundColor"] as! String)
        }else{
            facematch.setFeedbackBackGroundColor("#C4C4C5")
        }
        if faceArgs["feedbackTextColor"] != nil{
            facematch.setFeedbackTextColor(faceArgs["feedbackTextColor"] as! String)
        }else{
            facematch.setFeedbackTextColor("#000000")
        }
        
        
        if faceArgs["setFeedbackTextSize"] != nil {
            facematch.setFeedbackTextSize((faceArgs["setFeedbackTextSize"] as? Float)!)
        }else{
            facematch.setFeedbackTextSize(Float(18.0))
        }
        if faceArgs["setFeedBackframeMessage"] != nil {
            facematch.setFeedBackframeMessage(faceArgs["setFeedBackframeMessage"] as! String)
        }else{
            facematch.setFeedBackframeMessage("Frame Your Face")
        }
        if faceArgs["setFeedBackAwayMessage"] != nil {
            facematch.setFeedBackAwayMessage(faceArgs["setFeedBackAwayMessage"] as! String)
        }else{
            facematch.setFeedBackAwayMessage("Move Phone Away")
        }
        if faceArgs["setFeedBackOpenEyesMessage"] != nil {
            facematch.setFeedBackOpenEyesMessage(faceArgs["setFeedBackOpenEyesMessage"] as! String)
        }else{
            facematch.setFeedBackOpenEyesMessage("Keep Open Your Eyes")
            
        }
        if faceArgs["setFeedBackCloserMessage"] != nil {
            facematch.setFeedBackCloserMessage(faceArgs["setFeedBackCloserMessage"] as! String)
        }else{
            facematch.setFeedBackCloserMessage("Move Phone Closer")
        }
        if faceArgs["setFeedBackCenterMessage"] != nil {
            facematch.setFeedBackCenterMessage(faceArgs["setFeedBackCenterMessage"] as! String)
        }else{
            facematch.setFeedBackCenterMessage("Center Your Face")
        }
        if faceArgs["setFeedbackMultipleFaceMessage"] != nil {
            facematch.setFeedbackMultipleFaceMessage(faceArgs["setFeedbackMultipleFaceMessage"] as! String)
        }else{
            facematch.setFeedbackMultipleFaceMessage("Multiple face detected")
        }
        if faceArgs["setFeedBackFaceSteadymessage"] != nil {
            facematch.setFeedBackFaceSteadymessage(faceArgs["setFeedBackFaceSteadymessage"] as! String)
        }else{
            facematch.setFeedBackFaceSteadymessage("Keep Your Head Straight")
        }
        if faceArgs["setFeedBackLowLightMessage"] != nil {
            facematch.setFeedBackFaceSteadymessage(faceArgs["setFeedBackLowLightMessage"] as! String)
        }else{
            facematch.setFeedBackLowLightMessage("Low light detected")
        }
        if faceArgs["setFeedBackBlurFaceMessage"] != nil {
            facematch.setFeedBackBlurFaceMessage(faceArgs["setFeedBackBlurFaceMessage"] as! String)
        }else{
            facematch.setFeedBackBlurFaceMessage("Blur detected over face")
        }
        if faceArgs["setFeedBackGlareFaceMessage"] != nil {
            facematch.setFeedBackGlareFaceMessage(faceArgs["setFeedBackGlareFaceMessage"] as! String)
        }else{
            facematch.setFeedBackGlareFaceMessage("Glare detected")
        }
        if faceArgs["setBlurPercentage"] != nil {
            facematch.setBlurPercentage(faceArgs["setBlurPercentage"] as! Int32)
        }else{
            facematch.setBlurPercentage(80) // set blure percentage -1 to remove this filter
        }
        if faceArgs["setGlarePercentage_0"] != nil && faceArgs["setGlarePercentage_1"] != nil {
            facematch.setGlarePercentage(faceArgs["setGlarePercentage_0"] as! Int32, faceArgs["setGlarePercentage_1"] as! Int32)
        }else{
            facematch.setGlarePercentage(-1, -1)
        }
        
//        print("test:- \(faceArgs["isShowLogo"])")
        if faceArgs["isShowLogo"] != nil &&
            faceArgs["isShowLogo"] as! Int == 0{
            facematch.hideLogo(true)
        }else{
            facematch.hideLogo(false)
        }
        //        facematch.setBackGroundColor("#C4C4C5")
        //        facematch.setCloseIconColor("#000000")
        //        facematch.setFeedbackBackGroundColor("#C4C4C5")
        //        facematch.setFeedbackTextColor("#000000")
   
        //        facematch.setFeedbackTextSize(Float(18.0))
        //        facematch.setFeedBackframeMessage("Frame Your Face")
        //        facematch.setFeedBackAwayMessage("Move Phone Away")
        //        facematch.setFeedBackOpenEyesMessage("Keep Open Your Eyes")
        //        facematch.setFeedBackCloserMessage("Move Phone Closer")
        //        facematch.setFeedBackCenterMessage("Center Your Face")
        //        facematch.setFeedbackMultipleFaceMessage("Multiple face detected")
        //        facematch.setFeedBackFaceSteadymessage("Keep Your Head Straight")
        //        facematch.setFeedBackLowLightMessage("Low light detected")
        //        facematch.setFeedBackBlurFaceMessage("Blur detected over face")
        //        facematch.setFeedBackGlareFaceMessage("Glare detected")
        //        // 0 for clean face and 100 for Blurry face
        //        facematch.setBlurPercentage(80) // set blure percentage -1 to remove this filter
        //        // Set min and max percentage for glare
        //        facematch.setGlarePercentage(-1, -1) //set glaremin -1 to remove this filter
    }
    
    @IBAction func baAtn(_ sender: UIButton) {
        closeMe()
    }
    //    func startFC() {
    //        let facematch = Facematch()
    //        // To customize your screen theme and feed back messages
    //        facematch.setBackGroundColor(FaceMatchConfigs.backgroundColor)
    //        if livenessConfigs["livenessBackground"] != nil {
    //            facematch.setBackGroundColor(livenessConfigs["livenessBackground"] as! String)
    //        }
    //        facematch.setCloseIconColor(LivenessConfigs.livenessCloseIconColor)
    //        if livenessConfigs["livenessCloseIconColor"] != nil {
    //            facematch.setCloseIconColor(livenessConfigs["livenessCloseIconColor"] as! String)
    //        }
    //        facematch.setFeedbackBackGroundColor(LivenessConfigs.livenessfeedbackBackground)
    //        if livenessConfigs["livenessfeedbackBackground"] != nil {
    //            facematch.setFeedbackBackGroundColor(livenessConfigs["livenessfeedbackBackground"] as! String)
    //        }
    //        facematch.setFeedbackTextColor(LivenessConfigs.livenessfeedbackTextColor)
    //        if livenessConfigs["livenessfeedbackTextColor"] != nil {
    //            facematch.setFeedbackTextColor(livenessConfigs["livenessfeedbackTextColor"] as! String)
    //        }
    //        facematch.setFeedbackTextSize(Float(LivenessConfigs.feedbackTextSize))
    //        if livenessConfigs["feedbackTextSize"] != nil {
    //            facematch.setFeedbackTextSize(livenessConfigs["feedbackTextSize"] as! Float)
    //        }
    //        facematch.setFeedBackframeMessage(LivenessConfigs.feedBackframeMessage)
    //        if livenessConfigs["feedBackframeMessage"] != nil {
    //            facematch.setFeedBackframeMessage(livenessConfigs["feedBackframeMessage"] as! String)
    //        }
    //        facematch.setFeedBackAwayMessage(LivenessConfigs.feedBackAwayMessage)
    //        if livenessConfigs["feedBackAwayMessage"] != nil {
    //            facematch.setFeedBackAwayMessage(livenessConfigs["feedBackAwayMessage"] as! String)
    //        }
    //        facematch.setFeedBackOpenEyesMessage(LivenessConfigs.feedBackOpenEyesMessage)
    //        if livenessConfigs["feedBackOpenEyesMessage"] != nil {
    //            facematch.setFeedBackOpenEyesMessage(livenessConfigs["feedBackOpenEyesMessage"] as! String)
    //        }
    //        facematch.setFeedBackCloserMessage(LivenessConfigs.feedBackCloserMessage)
    //        if livenessConfigs["feedBackCloserMessage"] != nil {
    //            facematch.setFeedBackCloserMessage(livenessConfigs["feedBackCloserMessage"] as! String)
    //        }
    //        facematch.setFeedBackCenterMessage(LivenessConfigs.feedBackCenterMessage)
    //        if livenessConfigs["feedBackCenterMessage"] != nil {
    //            facematch.setFeedBackCenterMessage(livenessConfigs["feedBackCenterMessage"] as! String)
    //        }
    //        facematch.setFeedbackMultipleFaceMessage(LivenessConfigs.feedBackMultipleFaceMessage)
    //        if livenessConfigs["feedBackMultipleFaceMessage"] != nil {
    //            facematch.setFeedbackMultipleFaceMessage(livenessConfigs["feedBackMultipleFaceMessage"] as! String)
    //        }
    //        facematch.setFeedBackFaceSteadymessage(LivenessConfigs.feedBackHeadStraightMessage)
    //        if livenessConfigs["feedBackHeadStraightMessage"] != nil {
    //            facematch.setFeedBackFaceSteadymessage(livenessConfigs["feedBackHeadStraightMessage"] as! String)
    //        }
    //        facematch.setFeedBackLowLightMessage(LivenessConfigs.feedBackLowLightMessage)
    //        if livenessConfigs["feedBackLowLightMessage"] != nil {
    //            facematch.setFeedBackLowLightMessage(livenessConfigs["feedBackLowLightMessage"] as! String)
    //        }
    //        facematch.setFeedBackBlurFaceMessage(LivenessConfigs.feedBackBlurFaceMessage)
    //        if livenessConfigs["feedBackBlurFaceMessage"] != nil {
    //            facematch.setFeedBackBlurFaceMessage(livenessConfigs["feedBackBlurFaceMessage"] as! String)
    //        }
    //        facematch.setFeedBackGlareFaceMessage(LivenessConfigs.feedBackGlareFaceMessage)
    //        if livenessConfigs["feedBackGlareFaceMessage"] != nil {
    //            facematch.setFeedBackGlareFaceMessage(livenessConfigs["feedBackGlareFaceMessage"] as! String)
    //        }
    ////        facematch.setFeedBackProcessingMessage(LivenessConfigs.feedBackProcessingMessage)
    ////        if livenessConfigs.index(forKey: "feedBackProcessingMessage") != nil {
    ////            facematch.setFeedBackProcessingMessage(livenessConfigs["feedBackProcessingMessage"] as! String)
    ////        }
    ////        facematch.isShowLogoImage(LivenessConfigs.isShowLogo)
    ////        if livenessConfigs.index(forKey: "isShowLogo") != nil {
    ////            facematch.isShowLogoImage(livenessConfigs["isShowLogo"] as! Bool)
    ////        }
    ////        facematch.setLogoImage("ic_logo.png")
    //
    //        // 0 for clean face and 100 for Blurry face
    //        facematch.setBlurPercentage(Int32(LivenessConfigs.setBlurPercentage)) // set blure percentage -1 to remove this filter
    //
    //        if livenessConfigs["setBlurPercentage"] != nil {
    //            facematch.setBlurPercentage(livenessConfigs["setBlurPercentage"] as! Int32)
    //        }
    //
    //        var glarePerc0 = Int32(LivenessConfigs.setGlarePercentage_0)
    //        if livenessConfigs["setGlarePercentage_0"] != nil {
    //            glarePerc0 = livenessConfigs["setGlarePercentage_0"] as! Int32
    //        }
    //        var glarePerc1 = Int32(LivenessConfigs.setGlarePercentage_1)
    //        if livenessConfigs["setGlarePercentage_1"] != nil {
    //            glarePerc1 = livenessConfigs["setGlarePercentage_1"] as! Int32
    //        }
    //        // Set min and max percentage for glare
    //        facematch.setGlarePercentage(glarePerc0, glarePerc1) //set glaremin -1 and glaremax -1 to remove this filter
    //        // Do any additional setup after loading the view.
    //        facematch.setFacematch(self)
    //    }
    
    func imageCamera1() {
        DispatchQueue.main.asyncAfter(deadline: .now(), execute: {
            self.facematch.setFacematch(self)
        })
        selectFirstImage = true
        
    }
    func imageGallery1() {
        self.openPhotosLibrary(_isFirst: true)
    }
    
    func imageCamera2() {
        DispatchQueue.main.asyncAfter(deadline: .now(), execute: {
            self.facematch.setFacematch(self)
            self.view.setNeedsLayout()
        })
        selectFirstImage = false
        
    }
    
    func imageGallery2() {
        self.openPhotosLibrary(_isFirst: false)
    }
    
    func openPhotosLibrary(_isFirst: Bool){
        //Check camera permission
        let photos = PHPhotoLibrary.authorizationStatus()
        if photos == .denied {
            DispatchQueue.main.async {
                let alert = UIAlertController(title: "AccuraFrame", message: "It looks like your privacy settings are preventing us from accessing your Photos", preferredStyle: .alert);
                let defaultAction = UIAlertAction(title: "OK", style: .default) { _ in
                    if #available(iOS 10.0, *) {
                        UIApplication.shared.open(URL(string: UIApplication.openSettingsURLString)!, options: [:], completionHandler: nil)
                    } else {
                        UIApplication.shared.openURL(URL(string: UIApplication.openSettingsURLString)!)
                    }
                }
                alert.addAction(defaultAction)
                self.present(alert, animated: true, completion: nil)
            }
        }
        if photos == .denied || photos == .notDetermined{
            PHPhotoLibrary.requestAuthorization({status in
                if status == .authorized{
                    DispatchQueue.main.async {
                        self.openGallary(_isFirst)
                    }
                    
                }
            })
        }else if photos == .authorized{
            DispatchQueue.main.async {
                self.openGallary(_isFirst)
            }
        }
    }
    
    
    
    func openGallary(_ isFirst: Bool){
        if #available(iOS 14, *) {
            selectFirstImage = isFirst
            // using PHPickerViewController
            var config = PHPickerConfiguration(photoLibrary: PHPhotoLibrary.shared())
            config.selectionLimit = 1
            config.filter = .images
            config.preferredAssetRepresentationMode = .current
            let picker = PHPickerViewController(configuration: config)
            picker.delegate = self
            picker.presentationController?.delegate = self
            self.present(picker, animated: true, completion: nil)
            
        } else {
            selectFirstImage = isFirst
            self.imagePicker.sourceType = .photoLibrary
            self.imagePicker.allowsEditing = false
            self.present(self.imagePicker, animated: true, completion: nil)
        }
        
    }
    
    //MARK:- Image Rotation
    @objc func loadPhotoCaptured() {
        let img = allImageViewsSubViews(imagePicker.viewControllers.first?.view)?.last
        if img != nil {
            if let imgView: UIImageView = img as? UIImageView{
                imagePickerController(imagePicker, didFinishPickingMediaWithInfo: convertToUIImagePickerControllerInfoKeyDictionary([convertFromUIImagePickerControllerInfoKey(UIImagePickerController.InfoKey.originalImage) : imgView.image!]))
            }
        } else {
            imagePicker.dismiss(animated: true)
        }
    }
    
    func allImageViewsSubViews(_ view: UIView?) -> [AnyHashable]? {
        var arrImageViews: [AnyHashable] = []
        if (view is UIImageView) {
            if let view = view {
                arrImageViews.append(view)
            }
        } else {
            for subview in view?.subviews ?? [] {
                if let all = allImageViewsSubViews(subview) {
                    arrImageViews.append(contentsOf: all)
                }
            }
        }
        return arrImageViews
    }
    
    
    //MARK:- ImagePicker delegate
    func imagePickerController(_ picker: UIImagePickerController,
                               didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey : Any]) {
        // Local variable inserted by Swift 4.2 migrator.
        let info = convertFromUIImagePickerControllerInfoKeyDictionary(info)
        
        
        dismiss(animated: true, completion: nil)
//        ProgressHUD.show("Loading...")
//        SVProgressHUD.show(withStatus: "Loading...")
        DispatchQueue.global(qos: .background).async {
            guard var originalImage = info[convertFromUIImagePickerControllerInfoKey(UIImagePickerController.InfoKey.originalImage)] as? UIImage else { return }
            
            
            //Image Resize
            let ratio = CGFloat(originalImage.size.width) / originalImage.size.height
            originalImage = self.compressimage(with: originalImage, convertTo: CGSize(width: 600 * ratio, height: 600))!
            
//            let compressData = UIImage(data: originalImage.jpegData(compressionQuality: 1.0)!)
            DispatchQueue.main.asyncAfter(deadline: .now(), execute: {
                //                self.setFaceRegion(compressData!)//Set FaceMatch score
//                SVProgressHUD.dismiss()
//                ProgressHUD.dismiss()
            })
        }
        
    }
    
    @available(iOS 14, *)
    func picker(_ picker: PHPickerViewController, didFinishPicking results: [PHPickerResult]) {
        dismiss(animated: true, completion: nil)
        guard !results.isEmpty else {
            callBack!(["Failed to get Facematch. Please try again", NSNull()])
            EngineWrapper.faceEngineClose()
            closeMe()
            return
        }
        // request image urls
        for result in results {
            result.itemProvider.loadObject(ofClass: UIImage.self, completionHandler: { (object, error) in
                if let image = object as? UIImage {
                    DispatchQueue.main.async { [self] in
                        // Use UIImage
                        let ratio = CGFloat(image.size.width) / image.size.height
                        let originalImage = self.compressimage(with: image, convertTo: CGSize(width: 600 * ratio, height: 600))!
                        
                        let compressData = UIImage(data: originalImage.jpegData(compressionQuality: 1.0)!)
                        lvfm.face1Detect = EngineWrapper.detectSourceFaces(compressData)
                        
                        //                        self.setFaceRegion(compressData!)//Set FaceMatch score
//                        SVProgressHUD.dismiss()
//                        ProgressHUD.dismiss()

                        var result:[String:Any] = [:]
                        result["Image"] = compressData
                        if result.index(forKey: "Image") != nil {
                            let img1 = AccurascanKyc.getImageUri(img: compressData!, name: nil)
                            result["Image"] = img1
                            if(lvfm.No == 1){
                                lvfm.face1 = compressData
                                if lvfm.face1 != nil && lvfm.face2 != nil {
                                    lvfm.face1Detect = EngineWrapper.detectSourceFaces(lvfm.face1)
                                    lvfm.face2Detect = EngineWrapper.detectTargetFaces(lvfm.face2, feature1: lvfm.face1Detect?.feature)
                                    let fmSore = EngineWrapper.identify(lvfm.face1Detect?.feature, featurebuff2: lvfm.face2Detect?.feature)
                                    let twoDecimalPlaces = String(format: "%.2f", fmSore*100)//Match score Convert Float Value
                                    result["score"] = twoDecimalPlaces
                                }
                            }
                            if lvfm.No == 2 {
                                lvfm.face2 = compressData
                                if lvfm.face1 != nil && lvfm.face2 != nil {
                                    lvfm.face1Detect = EngineWrapper.detectSourceFaces(lvfm.face1)
                                    lvfm.face2Detect = EngineWrapper.detectTargetFaces(lvfm.face2, feature1: lvfm.face1Detect?.feature)
                                    let fmSore = EngineWrapper.identify(lvfm.face1Detect?.feature, featurebuff2: lvfm.face2Detect?.feature)
                                    let twoDecimalPlaces = String(format: "%.2f", fmSore*100)//Match score Convert Float Value
                                    result["score"] = twoDecimalPlaces
                                }
                            }
                            callBack!([NSNull(),AccurascanKyc.convertJSONString(results:result)])
                            EngineWrapper.faceEngineClose()
                            closeMe()
                        } else {
                            
                            callBack!(["Failed to get Facematch. Please try again", NSNull()])
                            EngineWrapper.faceEngineClose()
                            closeMe()
                        }
                    }
                }
            })
        }
    }
    
    func presentationControllerWillDismiss(_ presentationController: UIPresentationController) {
//        print("The user began to swipe down to dismiss.")
    }

    func presentationControllerDidDismiss(_ presentationController: UIPresentationController) {
//        print("The dismissal animation finished after the user swiped down.")
        closeMe()
        // This is probably where you want to put your code that you want to call.
    }

    
    func facematchData(_ FaceImage: UIImage!) {
//        SVProgressHUD.show(withStatus: "Loading...")
//        ProgressHUD.show("Loading...")
        DispatchQueue.global(qos: .background).async { [self] in
            var originalImage = FaceImage
            
            //Image Resize
            let ratio = CGFloat(FaceImage.size.width) / originalImage!.size.height
            originalImage = self.compressimage(with: originalImage, convertTo: CGSize(width: 600 * ratio, height: 600))!
            var result:[String:Any] = [:]
            let compressData = UIImage(data: FaceImage.jpegData(compressionQuality: 1.0)!)
            //            DispatchQueue.main.asyncAfter(deadline: .now(), execute: { [self] in
            //                self.setFaceRegion(compressData!)//Set FaceMatch score
            //                gl.face1Detect = EngineWrapper.detectSourceFaces(compressData)
//            SVProgressHUD.dismiss()
//            ProgressHUD.dismiss()

            //            })
            let img1 = AccurascanKyc.getImageUri(img: compressData!, name: nil)
            result["Image"] = img1
            if(lvfm.No == 1){
                lvfm.face1 = compressData
                if lvfm.face1 != nil && lvfm.face2 != nil {
                    lvfm.face1Detect = EngineWrapper.detectSourceFaces(lvfm.face1)
                    lvfm.face2Detect = EngineWrapper.detectTargetFaces(lvfm.face2, feature1: lvfm.face1Detect?.feature)
                    let fmSore = EngineWrapper.identify(lvfm.face1Detect?.feature, featurebuff2: lvfm.face2Detect?.feature)
                    let twoDecimalPlaces = String(format: "%.2f", fmSore*100)//Match score Convert Float Value
                    result["score"] = twoDecimalPlaces
                }
            }
            if lvfm.No == 2 {
                lvfm.face2 = compressData
                if lvfm.face1 != nil && lvfm.face2 != nil {
                    lvfm.face1Detect = EngineWrapper.detectSourceFaces(lvfm.face1)
                    lvfm.face2Detect = EngineWrapper.detectTargetFaces(lvfm.face2, feature1: lvfm.face1Detect?.feature)
                    let fmSore = EngineWrapper.identify(lvfm.face1Detect?.feature, featurebuff2: lvfm.face2Detect?.feature)
                    let twoDecimalPlaces = String(format: "%.2f", fmSore*100)//Match score Convert Float Value
                    result["score"] = twoDecimalPlaces
                    print(twoDecimalPlaces)
                }
                
            }
            callBack!([NSNull(),AccurascanKyc.convertJSONString(results:result)])
            EngineWrapper.faceEngineClose()
            closeMe()
        }
    }
    
    
    func compressimage(with image: UIImage?, convertTo size: CGSize) -> UIImage? {
        UIGraphicsBeginImageContext(size)
        image?.draw(in: CGRect(x: 0, y: 0, width: size.width, height: size.height))
        let destImage = UIGraphicsGetImageFromCurrentImageContext()
        UIGraphicsEndImageContext()
        return destImage
    }
    fileprivate func convertFromUIImagePickerControllerInfoKeyDictionary(_ input: [UIImagePickerController.InfoKey: Any]) -> [String: Any] {
        return Dictionary(uniqueKeysWithValues: input.map {key, value in (key.rawValue, value)})
    }
    
}

// Helper function inserted by Swift 4.2 migrator.
fileprivate func convertToUIImagePickerControllerInfoKeyDictionary(_ input: [String: Any]) -> [UIImagePickerController.InfoKey: Any] {
    return Dictionary(uniqueKeysWithValues: input.map { key, value in (UIImagePickerController.InfoKey(rawValue: key), value)})
}

// Helper function inserted by Swift 4.2 migrator.
fileprivate func convertFromUIImagePickerControllerInfoKey(_ input: UIImagePickerController.InfoKey) -> String {
    return input.rawValue
}

extension UIImage {
    func resizeWithPercent(percentage: CGFloat) -> UIImage? {
        let imageView = UIImageView(frame: CGRect(origin: .zero, size: CGSize(width: size.width * percentage, height: size.height * percentage)))
        imageView.contentMode = .scaleAspectFit
        imageView.image = self
        UIGraphicsBeginImageContextWithOptions(imageView.bounds.size, false, scale)
        guard let context = UIGraphicsGetCurrentContext() else { return nil }
        imageView.layer.render(in: context)
        guard let result = UIGraphicsGetImageFromCurrentImageContext() else { return nil }
        UIGraphicsEndImageContext()
        return result
    }
    func resizeWithWidth(width: CGFloat, height:CGFloat) -> UIImage? {
        let imageView = UIImageView(frame: CGRect(origin: .zero, size: CGSize(width: width, height: height)))
        imageView.contentMode = .scaleAspectFit
        imageView.image = self
        UIGraphicsBeginImageContextWithOptions(imageView.bounds.size, false, scale)
        guard let context = UIGraphicsGetCurrentContext() else { return nil }
        imageView.layer.render(in: context)
        guard let result = UIGraphicsGetImageFromCurrentImageContext() else { return nil }
        UIGraphicsEndImageContext()
        return result
    }
    func resizeWithWidth(width: CGFloat) -> UIImage? {
        let imageView = UIImageView(frame: CGRect(origin: .zero, size: CGSize(width: width, height: CGFloat(ceil(width/size.width * size.height)))))
        imageView.contentMode = .scaleAspectFit
        imageView.image = self
        UIGraphicsBeginImageContextWithOptions(imageView.bounds.size, false, scale)
        guard let context = UIGraphicsGetCurrentContext() else { return nil }
        imageView.layer.render(in: context)
        guard let result = UIGraphicsGetImageFromCurrentImageContext() else { return nil }
        UIGraphicsEndImageContext()
        return result
    }
    
    
}

extension FMController: FacematchData{
    func facematchViewDisappear() {
        closeMe()
    }
    
    
}

